sdocp('sdoc::js::core/caterwaul.macroexpander.hashed', 'Hashed macroexpander.\nThe naive macroexpander, implemented in sdoc::js::core/caterwaul.macroexpander.naive, takes linear time both in the number of syntax nodes and the number of macros. For potentially deep\npattern trees this becomes too slow for regular use. This macroexpander uses adaptive hashing to optimize lookups against the macro table, eliminating most of the checks that would have\nfailed.\n\n  Algorithm and use case analysis.\n  For n syntax nodes and k macro patterns, a naive approach performs O(nk) tree-match operations. Each match is O(n) in the complexity of the pattern tree. At first it seemed like this would\n  scale reasonably well, but version 0.6.7 of Caterwaul performed 750000 tree-match operations to load just the standard libraries. This ended up taking several seconds on some runtimes.\n\n  Hashing is an attractive solution because of the way macros are usually structured. It\'s unfortunate that the space of operator nodes is limited, but several different hashes in combination\n  can reduce the pattern space considerably. For example, here are some of the patterns used by the standard library:\n\n  | (* (l) ([] ([ (_)) (_)))\n    (* (let) ([] ([ (_)) (_)))\n    (, (_) (* (where) ([ (_))))\n    (, (_) ([] (unless) (_)))\n    (, (_) ([] (when) (_)))\n    (, (_) ([] (where) (_)))\n    (/ (/ (_) (mb)) (_))\n    (/ (_) ([] (. (cpb) (_)) (_)))\n    (/ (_) ([] (. (cps) (_)) (_)))\n    (/ (_) ([] (. (re) (_)) (_)))\n    (/ (_) ([] (. (se) (_)) (_)))\n\n  Just partitioning on the top node can save on average between 1/2 and 1/3 of the macro lookups. It\'s possible to do better though. An alternative strategy is to dive into the tree until we\n  hit a non-wildcard identifier and record its path. For example:\n\n  | (* (l) ([] ([ (_)) (_)))            -> l, [0]\n    (* (let) ([] ([ (_)) (_)))          -> let, [0]\n    (, (_) (* (where) ([ (_))))         -> where, [1][0]\n    (, (_) ([] (unless) (_)))           -> unless, [1][0]\n    (, (_) ([] (when) (_)))             -> when, [1][0]\n    (, (_) ([] (where) (_)))            -> where, [1][0]\n    (/ (/ (_) (mb)) (_))                -> mb, [0][1]\n    (/ (_) ([] (. (cpb) (_)) (_)))      -> cpb, [1][0][0]\n    (/ (_) ([] (. (cps) (_)) (_)))      -> cps, [1][0][0]\n    (/ (_) ([] (. (re) (_)) (_)))       -> re, [1][0][0]\n    (/ (_) ([] (. (se) (_)) (_)))       -> se, [1][0][0]\n\n  Note that we can\'t apply the same transformation to actual syntax nodes (since they may have identifiers all over the place), but that\'s ok. Each element in the table has a cost of\n  computation (more node traversal costs more), and it has a certain discrimination benefit (it reduces the number of comparisons we\'ll have to do later). Implicit in the discrimination\n  benefit is also the fact that deeper identifier embedding in the pattern indicates a pattern that is more difficult to reject quickly. This follows because there are many more identifiers\n  than operators, so statistically we expect to reject more patterns based on failed identifier matches than failed operator matches.\n\n  Implementation approaches.\n  Obviously it isn\'t terribly feasible to hash every syntax node in every way, since we\'ll discover useful information about it just by querying its data. At that point we will have\n  partitioned the macro-space to a smaller subset, and some other test will then serve to partition it further. Taking the macro list above, for example, suppose we\'re matching against the\n  tree (, (+ (x) (1)) (* (where) ([ (= (x) (5))))). (This corresponds to the code \'x + 1, where*[x = 5]\'.) Given the comma, we can immediately reduce the search space to querying for [1][0] to\n  find out which macro, if any, we should try to match against. After that we can verify the other nodes and construct the match array.\n\n  In general this procedure would be quite slow; there\'s a lot of decision-making going on here. However, this overhead vanishes if, rather than using higher-order logic to construct the match\n  function, we instead compile one tailored to the macro set. (Caterwaul /is/ a compiler, after all :). In the case of this macro set we\'d have something like this:\n\n  | var t1 = tree;\n    if (! t1) return false;             // Fail quickly if nodes don\'t exist\n    switch (t1.data.charCodeAt(0)) {    // Predication on operator, so only first character\n      case \'*\'.charCodeAt(0):           // Replaced with actual character code, of course\n        var t2 = t1[0];\n        if (! t2) return false;\n        switch (t2.data) {              // Predication on identifier, so whole string\n          case \'let\':\n            var t3 = t1[1];\n            if (! t3) return false;\n            var t4 = t3[0];\n            if (! t4) return false;\n            if (t3.data === \'[]\' && t4.data === \'[\') {\n              var p = [t4[0], t3[1]];\n              return macroexpander_1.apply(this, p) || macroexpander_2.apply(this, p) || ...;\n            } else\n              return false;\n          // ...\n\n  This strategy is ideal because it performs inexpensive checks first, then dives down to do the more time-consuming ones. It also has the benefit that failover cases are still preserved; a\n  macro that returns a falsy replacement will trigger the next macro in the series, finally failing over to no alteration. This is done in what I believe to be the fastest possible way.');