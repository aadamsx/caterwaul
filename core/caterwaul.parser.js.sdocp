sdocp('sdoc::js::core/caterwaul.parser', 'Parsing.\nThere are two distinct parts to parsing Javascript. One is parsing the irregular statement-mode expressions such as \'if (condition) {...}\' and \'function f(x) {...}\'; the other is parsing\nexpression-mode stuff like arithmetic operators. In Rebase I tried to model everything as an expression, but that failed sometimes because it required that each operator have fixed arity. In\nparticular this was infeasible for keywords such as \'break\', \'continue\', \'return\', and some others (any of these can be nullary or unary). It also involved creating a bizarre hack for \'case\nx:\' inside a switch block. This hack made the expression passed in to \'case\' unavailable, as it would be buried in a \':\' node.\n\nCaterwaul fixes these problems by using a proper context-free grammar. However, it\'s much looser than most grammars because it doesn\'t need to validate anything. Correspondingly, it can be\nmuch faster as well. Instead of guessing and backtracking as a recursive-descent parser would, it classifies many different branches into the same basic structure and fills in the blanks. One\nexample of this is the () {} pair, which occurs in a bunch of different constructs, including function () {}, if () {}, for () {}, etc. In fact, any time a () group is followed by a {} group\nwe can grab the token that precedes () (along with perhaps one more in the case of function f () {}), and group that under whichever keyword is responsible.\n\n  Syntax folding.\n  The first thing to happen is that parenthetical, square bracket, and braced groups are folded up. This happens in a single pass that is linear in the number of tokens, and other foldable\n  tokens (including unary and binary operators) are indexed by associativity. The following pass runs through these indexes from high to low precedence and folds tokens into trees. By this\n  point all of the parentheticals have been replaced by proper nodes (here I include ?: groups in parentheticals, since they behave the same way). Finally, high-level rules are applied to the\n  remaining keywords, which are bound last. This forms a complete parse tree.\n\n  Doing all of this efficiently requires a linked list rather than an array. This gets built during the initial paren grouping stage. Arrays are used for the indexes, which are left-to-right\n  and are later processed in the order indicated by the operator associativity. That is, left-associative operators are processed 0 .. n and right associative are processed n .. 0. Keywords\n  are categorized by behavior and folded after all of the other operators. Semicolons are folded last, from left to right.\n\n  There are some corner cases due to Javascript\'s questionable heritage from C-style syntax. For example, most constructs take either syntax blocks or semicolon-delimited statements. Ideally,\n  else, while, and catch are associated with their containing if, do, and try blocks, respectively. This can be done easily, as the syntax is folded right-to-left. Another corner case would\n  come up if there were any binary operators with equal precedence and different associativity. Javascript doesn\'t have them however, and it wouldn\'t make much sense to; it would render\n  expressions such as \'a op1 b op2 c\' ambiguous if op1 and op2 shared precedence but each wanted to bind first. (I mention this because at first I was worried about it, but now I realize it\n  isn\'t an issue.)\n\n  Notationally (for easier processing later on), a distinction is made between invocation and grouping, and between dereferencing and array literals. Dereferencing and function invocation are\n  placed into their own operators, where the left-hand side is the thing being invoked or dereferenced and the right-hand side is the paren-group or bracket-group that is responsible for the\n  operation. Also, commas inside these groups are flattened into a single variadic (possibly nullary) comma node so that you don\'t have to worry about the tree structure. This is the case for\n  all left-associative operators; right-associative operators preserve their hierarchical folding.\n\n  Parse/lex shared logic.\n  Lexing Javascript is not entirely straightforward, primarily because of regular expression literals. The first implementation of the lexer got things right 99% of the time by inferring the\n  role of a / by its preceding token. The problem comes in when you have a case like this:\n\n  | if (condition) /foo/.test(x)\n\n  In this case, (condition) will be incorrectly inferred to be a regular expression (since the close-paren terminates an expression, usually), and /foo/ will be interpreted as division by foo. \n\n  We mark the position before a token and then just increment the position. The token, then, can be retrieved by taking a substring from the mark to the position. This eliminates the need for\n  intermediate concatenations. In a couple of cases I\'ve gone ahead and done them anyway -- these are for operators, where we grab the longest contiguous substring that is defined. I\'m not too\n  worried about the O(n^2) complexity due to concatenation; they\'re bounded by four characters.\n\n  OK, so why use charAt() instead of regular expressions? It\'s a matter of asymptotic performance. V8 implements great regular expressions (O(1) in the match length for the (.*)$ pattern), but\n  the substring() method is O(n) in the number of characters returned. Firefox implements O(1) substring() but O(n) regular expression matching. Since there are O(n) tokens per document of n\n  characters, any O(n) step makes lexing quadratic. So I have to use the only reliably constant-time method provided by strings, charAt() (or in this case, charCodeAt()).\n\n  Of course, building strings via concatenation is also O(n^2), so I also avoid that for any strings that could be long. This is achieved by using a mark to indicate where the substring\n  begins, and advancing i independently. The span between mark and i is the substring that will be selected, and since each substring both requires O(n) time and consumes n characters, the\n  lexer as a whole is O(n). (Though perhaps with a large constant.)\n\n    Precomputed table values.\n    The lexer uses several character lookups, which I\'ve optimized by using integer->boolean arrays. The idea is that instead of using string membership checking or a hash lookup, we use the\n    character codes and index into a numerical array. This is guaranteed to be O(1) for any sensible implementation, and is probably the fastest JS way we can do this. For space efficiency,\n    only the low 256 characters are indexed. High characters will trigger sparse arrays, which may degrade performance. (I\'m aware that the arrays are power-of-two-sized and that there are\n    enough of them, plus the right usage patterns, to cause cache line contention on most Pentium-class processors. If we are so lucky to have a Javascript JIT capable enough to have this\n    problem, I think we\'ll be OK.)\n\n    The lex_op table indicates which elements trigger regular expression mode. Elements that trigger this mode cause a following / to delimit a regular expression, whereas other elements would\n    cause a following / to indicate division. By the way, the operator ! must be in the table even though it is never used. The reason is that it is a substring of !==; without it, !== would\n    fail to parse. (See test/lex-neq-failure for examples.)\n\n     var lex_op = hash(\'. new ++ -- u++ u-- u+ u- typeof u~ u! ! * / % + - << >> >>> < > <= >= instanceof in == != === !== & ^ | && || ? = += -= *= /= %= &= |= ^= <<= >>= >>>= : , \' +\n                       \'return throw case var const break continue void else u; ;\'),\n\n      lex_table = function (s) {for (var i = 0, xs = [false]; i < 8; ++i) xs.push.apply(xs, xs); for (var i = 0, l = s.length; i < l; ++i) xs[s.charCodeAt(i)] = true; return xs},\n      lex_float = lex_table(\'.0123456789\'),    lex_decimal = lex_table(\'0123456789\'),  lex_integer = lex_table(\'0123456789abcdefABCDEFx\'),  lex_exp = lex_table(\'eE\'),\n      lex_space = lex_table(\' \\n\\r\\t\'),        lex_bracket = lex_table(\'()[]{}\'),       lex_opener = lex_table(\'([{\'),                    lex_punct = lex_table(\'+-*/%&|^!~=<>?:;.,\'),\n        lex_eol = lex_table(\'\\n\\r\'),     lex_regexp_suffix = lex_table(\'gims\'),          lex_quote = lex_table(\'\\\'"/\'),                   lex_slash = \'/\'.charCodeAt(0),\n       lex_star = \'*\'.charCodeAt(0),              lex_back = \'\\\\\'.charCodeAt(0),             lex_x = \'x\'.charCodeAt(0),                     lex_dot = \'.\'.charCodeAt(0),\n       lex_zero = \'0\'.charCodeAt(0),     lex_postfix_unary = hash(\'++ --\'),              lex_ident = lex_table(\'$_abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\'),\n\n    Parse data.\n    The lexer and parser aren\'t entirely separate, nor can they be considering the complexity of Javascript\'s grammar. The lexer ends up grouping parens and identifying block constructs such\n    as \'if\', \'for\', \'while\', and \'with\'. The parser then folds operators and ends by folding these block-level constructs.\n\n    parse_reduce_order = map(hash, [\'function\', \'( [ . [] ()\', \'new delete\', \'u++ u-- ++ -- typeof u~ u! u+ u-\', \'* / %\', \'+ -\', \'<< >> >>>\', \'< > <= >= instanceof in\', \'== != === !==\', \'&\',\n                                    \'^\', \'|\', \'&&\', \'||\', \'case\', \'?\', \'= += -= *= /= %= &= |= ^= <<= >>= >>>=\', \':\', \',\', \'return throw break continue void\', \'var const\',\n                                    \'if else try catch finally for switch with while do\', \';\']),\n\nparse_associates_right = hash(\'= += -= *= /= %= &= ^= |= <<= >>= >>>= ~ ! new typeof u+ u- -- ++ u-- u++ ? if else function try catch finally for switch case with while do\'),\n   parse_inverse_order = (function (xs) {for (var  o = {}, i = 0, l = xs.length; i < l; ++i) for (var k in xs[i]) has(xs[i], k) && (o[k] = i); return annotate_keys(o)}) (parse_reduce_order),\n   parse_index_forward = (function (rs) {for (var xs = [], i = 0, l = rs.length, _ = null; _ = rs[i], xs[i] = true, i < l; ++i)\n                                           for (var k in _) if (has(_, k) && (xs[i] = xs[i] && ! has(parse_associates_right, k))) break; return xs}) (parse_reduce_order),\n\n              parse_lr = hash(\'[] . () * / % + - << >> >>> < > <= >= instanceof in == != === !== & ^ | && || = += -= *= /= %= &= |= ^= <<= >>= >>>= , : ;\'),\n   parse_r_until_block = annotate_keys({\'function\':2, \'if\':1, \'do\':1, \'catch\':1, \'try\':1, \'for\':1, \'while\':1, \'with\':1}),\n         parse_accepts = annotate_keys({\'if\':\'else\', \'do\':\'while\', \'catch\':\'finally\', \'try\':\'catch\'}),  parse_invocation = hash(\'[] ()\'),\n      parse_r_optional = hash(\'return throw break continue else\'),  parse_also_expression = hash(\'function\'),    parse_r = hash(\'u+ u- u! u~ u++ u-- new typeof finally var const void delete\'),\n           parse_block = hash(\'; {\'),  parse_invisible = hash(\'i;\'),              parse_l = hash(\'++ --\'),   parse_group = annotate_keys({\'(\':\')\', \'[\':\']\', \'{\':\'}\', \'?\':\':\'}),\n parse_ambiguous_group = hash(\'[ (\'),    parse_ternary = hash(\'?\'),     parse_not_a_value = hash(\'function if for while catch\'),\n\n  Parse function.\n  As mentioned earlier, the parser and lexer aren\'t distinct. The lexer does most of the heavy lifting; it matches parens and brackets, arranges tokens into a hierarchical linked list, and\n  provides an index of those tokens by their fold order. It does all of this by streaming tokens into a micro-parser whose language is grouping and that knows about the oddities required to\n  handle regular expression cases. In the same function, though as a distinct case, the operators are folded and the syntax is compiled into a coherent tree form.\n\n  The input to the parse function can be anything whose toString() produces valid Javascript code.\n\n      parse = function (input) {\n\n    Lex variables.\n    s, obviously, is the string being lexed. mark indicates the position of the stream, while i is used for lookahead. The difference is later read into a token and pushed onto the result. c\n    is a temporary value used to store the current character code. re is true iff a slash would begin a regular expression. esc is a flag indicating whether the next character in a string or\n    regular expression literal is escaped. exp indicates whether we\'ve seen the exponent marker in a number. close is used for parsing single and double quoted strings; it contains the\n    character code of the closing quotation mark. t is the token to be processed.\n\n    Parse variables.\n    grouping_stack and gs_top are used for paren/brace/etc. matching. head and parent mark two locations in the linked syntax tree; when a new group is created, parent points to the opener\n    (i.e. (, [, ?, or {), while head points to the most recently added child. (Hence the somewhat complex logic in push().) indexes[] determines reduction order, and contains references to the\n    nodes in the order in which they should be folded. invocation_nodes is an index of the nodes that will later need to be flattened.\n\n    The push() function manages the mechanics of adding a node to the initial linked structure. There are a few cases here; one is when we\'ve just created a paren group and have no \'head\'\n    node; in this case we append the node as \'head\'. Another case is when \'head\' exists; in that case we update head to be the new node, which gets added as a sibling of the old head.\n\n        var s = input.toString(), mark = 0, c = 0, re = true, esc = false, dot = false, exp = false, close = 0, t = \'\', i = 0, l = s.length, cs = function (i) {return s.charCodeAt(i)},\n            grouping_stack = [], gs_top = null, head = null, parent = null, indexes = map(function () {return []}, parse_reduce_order), invocation_nodes = [], all_nodes = [],\n            new_node = function (n) {return all_nodes.push(n), n}, push = function (n) {return head ? head._sibling(head = n) : (head = n._append_to(parent)), new_node(n)};\n\n    Main lex loop.\n    This loop takes care of reading all of the tokens in the input stream. At the end, we\'ll have a linked node structure with paren groups. At the beginning, we set the mark to the current\n    position (we\'ll be incrementing i as we read characters), munch whitespace, and reset flags.\n\n        while ((mark = i) < l) {\n          while (lex_space[c = cs(i)] && i < l) mark = ++i;\n          esc = exp = dot = t = false;\n\n      Miscellaneous lexing.\n      This includes bracket resetting (the top case, where an open-bracket of any sort triggers regexp mode) and comment removal. Both line and block comments are removed by comparing against\n      lex_slash, which represents /, and lex_star, which represents *.\n\n            if                                        (lex_bracket[c])                                                                    {t = !! ++i; re = lex_opener[c]}\n       else if (c === lex_slash && cs(i + 1) === lex_star && (i += 2)) {while (++i < l && cs(i) !== lex_slash || cs(i - 1) !== lex_star);  t = !  ++i}\n       else if            (c === lex_slash && cs(i + 1) === lex_slash) {while                              (++i < l && ! lex_eol[cs(i)]);  t = false}\n\n      Regexp and string literal lexing.\n      These both take more or less the same form. The idea is that we have an opening delimiter, which can be ", \', or /; and we look for a closing delimiter that follows. It is syntactically\n      illegal for a string to occur anywhere that a slash would indicate division (and it is also illegal to follow a string literal with extra characters), so reusing the regular expression\n      logic for strings is not a problem. (This follows because we know ahead of time that the Javascript is valid.)\n\n       else if (lex_quote[c] && (close = c) && re && ! (re = ! (t = s.charAt(i)))) {while (++i < l && (c = cs(i)) !== close || esc)  esc = ! esc && c === lex_back;\n                                                                                    while     (++i < l && lex_regexp_suffix[cs(i)])                               ; t = true}\n\n      Numeric literal lexing.\n      This is far more complex than the above cases. Numbers have several different formats, each of which requires some custom logic. The reason we need to parse numbers so exactly is that it\n      influences how the rest of the stream is lexed. One example is \'0.5.toString()\', which is perfectly valid Javascript. What must be output here, though, is \'0.5\', \'.\', \'toString\', \'(\',\n      \')\'; so we have to keep track of the fact that we\'ve seen one dot and stop lexing the number on the second.\n\n      Another case is exponent-notation: 3.0e10. The hard part here is that it\'s legal to put a + or - on the exponent, which normally terminates a number. Luckily we can safely skip over any\n      character that comes directly after an E or e (so long as we\'re really in exponent mode, which I\'ll get to momentarily), since there must be at least one digit after an exponent.\n\n      The final case, which restricts the logic somewhat, is hexadecimal numbers. These also contain the characters \'e\' and \'E\', but we cannot safely skip over the following character, and any\n      decimal point terminates the number (since \'0x5.toString()\' is also valid Javascript). The same follows for octal numbers; the leading zero indicates that there will be no decimal point,\n      which changes the lex mode (for example, \'0644.toString()\' is valid).\n\n      So, all this said, there are different logic branches here. One handles guaranteed integer cases such as hex/octal, and the other handles regular numbers. The first branch is triggered\n      whenever a number starts with zero and is followed by \'x\' or a digit (for conciseness I call \'x\' a digit), and the second case is triggered when \'.\' is followed by a digit, or when a\n      digit starts.\n\n      A trivial change, using regular expressions, would reduce this logic significantly. I chose to write it out longhand because (1) it\'s more fun that way, and (2) the regular expression\n      approach has theoretically quadratic time in the length of the numbers, whereas this approach keeps things linear. Whether or not that actually makes a difference I have no idea.\n\n      Finally, in response to a recently discovered failure case, a period must be followed by a digit if it starts a number. The failure is the string \'.end\', which will be lexed as \'.en\',\n      \'d\' if it is assumed to be a floating-point number. (In fact, any method or property beginning with \'e\' will cause this problem.)\n\n       else if                  (c === lex_zero && lex_integer[cs(i + 1)]) {while (++i < l && lex_integer[cs(i)]); re = ! (t = true)}\n       else if (lex_float[c] && (c !== lex_dot || lex_decimal[cs(i + 1)])) {while (++i < l && (lex_decimal[c = cs(i)] || (dot ^ (dot |= c === lex_dot)) || (exp ^ (exp |= lex_exp[c] && ++i))));\n                                                                            while (i < l && lex_decimal[cs(i)]) ++i; re = ! (t = true)}\n\n      Operator lexing.\n      The \'re\' flag is reused here. Some operators have both unary and binary modes, and as a heuristic (which happens to be accurate) we can assume that anytime we expect a regular\n      expression, a unary operator is intended. The only exception are ++ and --, which are always unary but sometimes are prefix and other times are postfix. If re is true, then the prefix\n      form is intended; otherwise, it is postfix. For this reason I\'ve listed both \'++\' and \'u++\' (same for --) in the operator tables; the lexer is actually doing more than its job here by\n      identifying the variants of these operators.\n\n      The only exception to the regular logic happens if the operator is postfix-unary. (e.g. ++, --.) If so, then the re flag must remain false, since expressions like \'x++ / 4\' can be valid.\n\n       else if (lex_punct[c] && (t = re ? \'u\' : \'\', re = true)) {while (i < l && lex_punct[cs(i)] && has(lex_op, t + s.charAt(i)))  t += s.charAt(i++); re = ! has(lex_postfix_unary, t)}\n\n      Identifier lexing.\n      If nothing else matches, then the token is lexed as a regular identifier or Javascript keyword. The \'re\' flag is set depending on whether the keyword expects a value. The nuance here is\n      that you could write \'x / 5\', and it is obvious that the / means division. But if you wrote \'return / 5\', the / would be a regexp delimiter because return is an operator, not a value. So\n      at the very end, in addition to assigning t, we also set the re flag if the word turns out to be an operator.\n\n       else {while (++i < l && lex_ident[cs(i)]); re = has(lex_op, t = s.substring(mark, i))}\n\n      Token unification.\n      t will contain true, false, or a string. If false, no token was lexed; this happens when we read a comment, for example. If true, the substring method should be used. (It\'s a shorthand to\n      avoid duplicated logic.) For reasons that are not entirely intuitive, the lexer sometimes produces the artifact \'u;\'. This is never useful, so I have a case dedicated to removing it.\n\n        if (i === mark) throw new Error(\'Caterwaul lex error at "\' + s.substr(mark, 40) + \'" with leading context "\' + s.substr(mark - 40, 40) + \'" (probably a Caterwaul bug)\');\n        if (t === false) continue;\n        t = t === true ? s.substring(mark, i) : t === \'u;\' ? \';\' : t;\n\n      Grouping and operator indexing.\n      Now that we have a token, we need to see whether it affects grouping status. There are a couple of possibilities. If it\'s an opener, then we create a new group; if it\'s a matching closer\n      then we close the current group and pop out one layer. (We don\'t check for matching here. Any code provided to Caterwaul will already have been parsed by the host Javascript interpreter,\n      so we know that it is valid.)\n\n      All operator indexing is done uniformly, left-to-right. Note that the indexing isn\'t strictly by operator. It\'s by reduction order, which is arguably more important. That\'s what the\n      parse_inverse_order table does: it maps operator names to parse_reduce_order subscripts. (e.g. \'new\' -> 2.)\n\n        t === gs_top ? (grouping_stack.pop(), gs_top = grouping_stack[grouping_stack.length - 1], head = head ? head.p : parent, parent = null) :\n                       (has(parse_group, t) ? (grouping_stack.push(gs_top = parse_group[t]), parent = push(new_node(new syntax_node(t))), head = null) : push(new_node(new syntax_node(t))),\n                        has(parse_inverse_order, t) && indexes[parse_inverse_order[t]].push(head || parent));\n\n      Regexp flag special cases.\n      Normally a () group wraps an expression, so a following / would indicate division. The only exception to this is when we have a block construct; in this case, the next token appears in\n      statement-mode, which means that it begins, not modifies, a value. We\'ll know that we have such a case if (1) the immediately-preceding token is a close-paren, and (2) a block-accepting\n      syntactic form occurs to its left.\n\n      With all this trouble over regular expressions, I had to wonder whether it was possible to do it more cleanly. I don\'t think it is, unfortunately. Even lexing the stream backwards fails\n      to resolve the ambiguity:\n\n      | for (var k in foo) /foo/g.test(k) && bar();\n\n      In this case we won\'t know it\'s a regexp until we hit the \'for\' keyword (or perhaps \'var\', if we\'re being clever -- but a \'with\' or \'if\' would require complete lookahead). A perfectly\n      valid alternative parse, minus the \'for\' and \'var\', is this:\n\n      | ((k in foo) / (foo) / (g.test(k))) && bar();\n\n      The only case where reverse-lexing is useful is when the regexp has no modifiers.\n\n        re |= t === \')\' && head.l && has(parse_r_until_block, head.l.data)}\n\n    Operator fold loop.\n    This is the second major part of the parser. Now that we\'ve completed the lex process, we can fold operators and syntax, and take care of some exception cases.\n\n    First step: fold function literals, function calls, dots, and dereferences.\n    I\'m treating this differently from the generalized operator folding because of the syntactic inference required for call and dereference detection. Nothing has been folded at this point\n    (with the exception of paren groups, which is appropriate), so if the node to the left of any ( or [ group is an operator, then the ( or [ is really a paren group or array literal. If, on\n    the other hand, it is another value, then the group is a function call or a dereference. This folding goes left-to-right. The reason we also process dot operators is that they share the same\n    precedence as calls and dereferences. Here\'s what a () or [] transform looks like:\n\n    |   quux <--> foo <--> ( <--> bar                              quux <--> () <--> bar\n                            \\                                               /  \\                  <-- This can be done by saying _.l.wrap(new node(\'()\')).p.fold_r().\n                             bif <--> , <--> baz       -->               foo    (                     _.l.wrap() returns l again, .p gets the wrapping node, and fold_r adds a child to it.\n                                                                                 \\\n                                                                                  bif <--> , <--> baz\n\n    This is actually merged into the for loop below, even though it happens before other steps do (see \'Ambiguous parse groups\').\n\n    Second step: fold operators.\n    Now we can go through the list of operators, folding each according to precedence and associativity. Highest to lowest precedence here, which is just going forwards through the indexes[]\n    array. The parse_index_forward[] array indicates which indexes should be run left-to-right and which should go right-to-left.\n\n        for (var i = 0, l = indexes.length, forward, _; _ = indexes[i], forward = parse_index_forward[i], i < l; ++i)  \n          for (var j = forward ? 0 : _.length - 1, lj = _.length, inc = forward ? 1 : -1, node, data; node = _[j], data = node && node.data, forward ? j < lj : j >= 0; j += inc)\n\n      Binary node behavior.\n      The most common behavior is binary binding. This is the usual case for operators such as \'+\' or \',\' -- they grab one or both of their immediate siblings regardless of what they are.\n      Operators in this class are considered to be \'fold_lr\'; that is, they fold first their left sibling, then their right.\n\n            if (has(parse_lr, data)) node._fold_lr();\n\n      Ambiguous parse groups.\n      As mentioned above, we need to determine whether grouping constructs are invocations or real groups. This happens to take place before other operators are parsed (which is good -- that way\n      it reflects the precedence of dereferencing and invocation). The only change we need to make is to discard the explicit parenthetical or square-bracket grouping for invocations or\n      dereferences, respectively. It doesn\'t make much sense to have a doubly-nested structure, where we have a node for invocation and another for the group on the right-hand side of that\n      invocation. Better is to modify the group in-place to represent an invocation.\n\n      We can\'t solve this problem here, but we can solve it after the parse has finished. I\'m pushing these invocation nodes onto an index for the end.\n\n       else if (has(parse_ambiguous_group, data) && node.l && (node.l.data === \'.\' ||\n                     ! (has(lex_op, node.l.data) || has(parse_not_a_value, node.l.data))))  invocation_nodes.push(node.l._wrap(new_node(new syntax_node(data + parse_group[data]))).p._fold_r());\n\n      Unary left and right-fold behavior.\n      Unary nodes have different fold directions. In this case, it just determines which side we grab the node from. I\'m glad that Javascript doesn\'t allow stuff like \'++x++\', which would make\n      the logic here actually matter. Because there isn\'t that pathological case, exact rigidity isn\'t required.\n\n       else if (has(parse_l, data))  node._fold_l();\n       else if (has(parse_r, data))  node._fold_r();\n\n      Ternary operator behavior.\n      This is kind of interesting. If we have a ternary operator, then it will be treated first as a group; just like parentheses, for example. This is the case because the ternary syntax is\n      unambiguous for things in the middle. So, for example, \'3 ? 4 : 5\' initially parses out as a \'?\' node whose child is \'4\'. Its siblings are \'3\' and \'5\', so folding left and right is an\n      obvious requirement. The only problem is that the children will be in the wrong order. Instead of (3) (4) (5), we\'ll have (4) (3) (5). So after folding, we do a quick swap of the first two\n      to set the ordering straight.\n\n       else if (has(parse_ternary, data)) {node._fold_lr(); var temp = node[1]; node[1] = node[0]; node[0] = temp}\n\n      Grab-until-block behavior.\n      Not quite as simple as it sounds. This is used for constructs such as \'if\', \'function\', etc. Each of these constructs takes the form \'<construct> [identifier] () {}\', but they can also\n      have variants that include \'<construct> () {}\', \'<construct> () statement;\', and most problematically \'<construct> () ;\'. Some of these constructs also have optional child components; for\n      example, \'if () {} else {}\' should be represented by an \'if\' whose children are \'()\', \'{}\', and \'else\' (whose child is \'{}\'). The tricky part is that \'if\' doesn\'t accept another \'if\' as a\n      child (e.g. \'if () {} if () {}\'), nor does it accept \'for\' or any number of other things. This discrimination is encoded in the parse_accepts table.\n\n      There are some weird edge cases, as always. The most notable is what happens when we have nesting without blocks:\n\n      | if (foo) bar; else bif;\n\n      In this case we want to preserve the semicolon on the \'then\' block -- that is, \'bar;\' should be its child; so the semicolon is required. But the \'bif\' in the \'else\' case shouldn\'t have a\n      semicolon, since that separates top-level statements. Because desperate situations call for desperate measures, there\'s a hack specifically for this in the syntax tree serialization.\n\n      One more thing. Firefox rewrites syntax trees, and one of the optimizations it performs on object literals is removing quotation marks from regular words. This means that it will take the\n      object {\'if\': 4, \'for\': 1, etc.} and render it as {if: 4, for: 1, etc.}. As you can imagine, this becomes a big problem as soon as the word \'function\' is present in an object literal. To\n      prevent this from causing problems, I only collapse a node if it is not followed by a colon. (And the only case where any of these would legally be followed by a colon is as an object\n      key.)\n\n       else if (has(parse_r_until_block, data) && node.r && node.r.data !== \':\')  \n                                                 {for (var count = 0, limit = parse_r_until_block[data]; count < limit && node.r && ! has(parse_block, node.r.data); ++count) node._fold_r();\n                                                  node.r && node.r.data !== \';\' && node._fold_r();\n                                                  if (has(parse_accepts, data) && parse_accepts[data] === (node.r && node.r.r && node.r.r.data)) node._fold_r().pop()._fold_r();\n                                             else if (has(parse_accepts, data) && parse_accepts[data] === (node.r && node.r.data))               node._fold_r()}\n\n      Optional right-fold behavior.\n      The return, throw, break, and continue keywords can each optionally take an expression. If the token to the right is an expression, then we take it, but if the token to the right is a\n      semicolon then the keyword should be nullary.\n\n       else if (has(parse_r_optional, data))  node.r && node.r.data !== \';\' && node._fold_r();\n\n    Third step.\n    Find all elements with right-pointers and wrap them with semicolon nodes. This is necessary because of certain constructs at the statement-level don\'t use semicolons; they use brace syntax\n    instead. (e.g. \'if (foo) {bar} baz()\' is valid, even though no semicolon precedes \'baz()\'.) By this point everything else will already be folded. Note that this does some weird things to\n    associativity; in general, you can\'t make assumptions about the exact layout of semicolon nodes. Fortunately semicolon is associative, so it doesn\'t matter in practice. And just in case,\n    these nodes are \'i;\' rather than \';\', meaning \'inferred semicolon\' -- that way it\'s clear that they aren\'t original. (They also won\'t appear when you call toString() on the syntax tree.)\n\n        for (var i = all_nodes.length - 1, _; _ = all_nodes[i], i >= 0; --i)  _.r && _._wrap(new syntax_node(\'i;\')).p._fold_r();\n\n    Fourth step.\n    Flatten out all of the invocation nodes. As explained earlier, they are nested such that the useful data on the right is two levels down. We need to grab the grouping construct on the\n    right-hand side and remove it so that only the invocation or dereference node exists. During the parse phase we built an index of all of these invocation nodes, so we can iterate through\n    just those now. I\'m preserving the \'p\' pointers, though they\'re probably not useful beyond here.\n\n        for (var i = 0, l = invocation_nodes.length, _, child; _ = invocation_nodes[i], i < l; ++i) (child = _[1] = _[1][0]) && (child.p = _);\n\n        while (head.p) head = head.p;\n\n    Fifth step.\n    Prevent a space leak by clearing out all of the \'p\' pointers.\n\n        for (var i = all_nodes.length - 1; i >= 0; --i)  delete all_nodes[i].p;\n        return head};');